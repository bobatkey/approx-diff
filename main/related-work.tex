\section{Related work}
\label{sec:related-work}

\subsection{Stable Domain Theory}

Stable Domain Theory was originally proposed by \citet{berry79} as a refinement of domain theory aimed at
capturing the intensional behaviour of sequential programs, and elaborated on subsequently by \citet{berry82}
and \citet{amadio-curien}. Standard domain-theoretic models interpret programs as continuous functions,
preserving directed joins; Berry observed that this continuity condition alone is too permissive to model
sequentiality. Stability imposes additional constraints to reflect how functions preserve bounded meets of
approximants, effectively requiring that the evaluation of a function respect a specific computational order.
Though stable functions do not fully characterise sequentiality, because they admit $\mathrm{gustave}$-style
counterexamples (\exref{parallel-or}), they remain an appropriate notion for studying the sensitivity of a
program to partial data at a specific point.

Our use of Stable Domain Theory diverges from the traditional aim of modelling infinite or partial data,
however. Instead, we follow a line of work that uses partiality as a qualitative notion of approximation
suitable for provenance and program slicing (discussed in more detail in \secref{related-work:galois-slicing}
below). Paul Taylor’s characterisation of stable functions via local Galois connections on principle downsets
provides the semantic underpinning for the reverse maps used in Galois slicing~\cite{taylor99}. Our work
builds on these ideas by interpreting Galois slicing as a form of differentiable programming, using the
machinery of stable functions and Galois connections to present Galois slicing in a denotational style.

\subsection{Automatic Differentiation}

Automatic differentiation (AD), discussed in \secref{first-order:autodiff}, is the idea of computing
derivatives of functions expressed as programs by systematically applying the chain rule. The observation that
these derivative computations could be interleaved with the evaluation of the original program is due to
\citet{linnainmaa76}, who showed how the forward derivative $\pushf{f}_x$ of $f$ at a point $x$ could be
computed alongside $f(x)$ in a single pass, dramatically improving the efficiency of derivative evaluation
over symbolic or numerical differentiation. This insight became the foundation of forward-mode AD, which
underpins many optimisation and scientific computing tools, including JAX~\cite{jax2018github}.

More recent approaches to automatic differentiation have emphasised its semantic foundations.
\citet{elliott18} proposed a categorical model of AD that interprets programs not only as functions enriched
with their derivatives, giving a compositional account of differentiation based on duality and linear maps.
This line of work connects AD to denotational semantics, abstracting over operational concerns and enabling
principled extensions to differentiable programming languages. Vákár and
collaborators~\cite{vákár22,nunes2023} developed the CHAD framework, using Grothendieck constructions over
indexed categories to capture both values and their tangents in a compositional semantic structure. These
semantic perspectives shed light on the categorical structure of AD and guide the design of systems that
generalise beyond real analysis, including the application to data provenance and slicing explored in this
paper.

\subsection{\GPS}
\label{sec:related-work:galois-slicing}

\cite{perera12a,perera13,perera16d,ricciotti17,perera22,bond25}

\subsection{Tangent Categories}

\cite{cockett14,cockett18}

\subsection{Lens Categories}

\cite{spivak19}
