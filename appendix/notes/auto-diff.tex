\subsection{Automatic differentiation}

Write $\tangents_x(\RR^n)$ for the tangent space at a point $x \in \RR^n$. Then the \emph{forward derivative}
(tangent map or pushforward) $\pushf{f}_x$ of a differentiable function $f: \RR^m \to \RR^n$ at $x \in \RR^m$
is the unique linear map $\tangents_x(\RR^m) \linearto \tangents_{f(x)}(\RR^n)$ given by the Jacobean matrix
of $f$ at $x$. Since $\RR^n$ is flat, $\tangents_x(\RR^n)$ is naturally isomorphic to $\RR^n$, so in fact
$\pushf{f}_x$ is a linear map $\RR^m \linearto \RR^n$. The \emph{backward derivative} (cotangent map or
pullback) $\pullf{f}_x$ is the unique linear map $\RR^n \linearto \RR^m$ given by the transpose (adjoint) of
the Jacobean matrix of $f$ at $x$.

\subsubsection{Chain rule}

Suppose $f: \RR^m \to \RR^n$ and $g: \RR^n \to \RR^k$. For any $x \in \RR^m$ we can define the following
composite derivatives of $g \comp f: \RR^m \to \RR^k$:

\begin{itemize}
\item $\pushf{(g \comp f)}_x = \pushf{g}_{f(x)} \comp \pushf{f}_x$
\item $\pullf{(g \comp f)}_x = \pullf{f}_{x} \comp \pullf{g}_{f(x)}$
\end{itemize}

\begin{definition}[Forward mode automatic differentiation functor]
Define the functor $\tangents_*$ which sends every vector space $\RR^n$ to itself and every smooth map $f:
\RR^m \to \RR^n$ to the function $\tangents_*(f) = x \mapsto (f(x), \pushf{f}_x): \RR^m \to \RR^n \times (\RR^m
\linearto \RR^n)$, associating to every point its image in $f$ and the forward derivative of $f$ at that
point.
\end{definition}

For any map $f: \RR^m \to \RR^n \times (\RR^m \linearto \RR^n)$, write $f_1$ for $\pi_1 \comp f$ and $f_2$ for
$\pi_2 \comp f$. Regardless of whether the linear maps $\RR^m \linearto \RR^n$ happen to be derivatives, we
can define composition on such maps as:
\begin{align*}
(g \comp f)_1(x) &= f_1(g_1(x)) \\
(g \comp f)_2(x) &= g_2(f_1(x)) \comp f_2(x)
\end{align*}
\noindent and verify that $\tangents$ is indeed a functor. If $f_2(x)$ happens to be the derivative of $f_1$
at $x$ for any $x \in \RR^m$ and similarly for $g$ then the derivatives compose according to the forward chain
rule and $(g \comp f)_2(x)$ is the derivative of $(g \comp f)_1$ at $x$.

\begin{definition}[Reverse mode automatic differentiation functor]
Define the functor $\tangents^*$ which also sends every vector space $\RR^n$ to itself but which sends every
smooth map $f: \RR^m \to \RR^n$ to the function $\tangents^*(f) = x \mapsto (f(x), \pullf{f}_x): \RR^m \to
\RR^n \times (\RR^n \to \RR^m)$.
\end{definition}

\noindent and again we can define a notion of composition for such maps that respects the (backward) chain
rule.

\subsubsection{Automatic differentiation via the Grothendieck construction}

Let $X, Y, Z$ range over sets and $U, V, W$ range over finite vector spaces. Recall that the category
$\Fam(\FinVect) = \Grothendieck{}\Fam(-, \FinVect)$ (\defref{grothendieck:Grothendieck}) has:
\begin{itemize}
\item as objects, all pairs $(X, \{V_x\}_{x \in X})$ of a set $X$ and an object of $\Fam(X,\FinVect)$;
\item as morphisms $(X, \{V_x\}_{x \in X}) \to (Y, \{W_y\}_{y \in Y})$, all pairs $(f, \partial f)$ of
functions $f: X \to Y$ and families of morphisms $\{\partial f_x: V_x \to W_{f(x)}\}_{x \in X}$
in $\Fam(X,\FinVect)$.
\end{itemize}

Also recall that $\Fam(f, \FinVect)$ for any function $f: X \to Y$ is the reindexing functor $\reindex{-}{f}:
\Fam(Y, \FinVect) \to \Fam(X, \FinVect)$ sending any family of morphisms $\{g_y\}_{y \in Y}$ to
$\{g_{f(x)}\}_{x \in X}$. Then for any $(f, \partial f): (X, \{V_x\}_{x \in X}) \to (Y, \{W_y\}_{y \in Y})$
and $(g, \partial g): (Y, \{W_y\}_{y \in Y}) \to (Z, \{U_z\}_{z \in Z})$ we have, for any $x \in X$:
\begin{itemize}
\item $\partial f_x: V_x \to W_{f(x)}$;
\item $\reindex{\partial g}{f}_x: W_{f(x)} \to U_{g(f(x))}$.
\end{itemize}

\noindent The composition $(g, \partial g) \comp (f, \partial f): (X, \{V_x\}_{x \in X}) \to (Z, \{U_z\}_{z
\in Z})$ is thus given by $(g \comp f, \reindex{\partial g}{f} \comp \partial f)$.
